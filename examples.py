# -*- coding: utf-8 -*-
"""examples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ayr5RtSR6nsAYmXRo1WKgE1J-MHhgVCX
"""

import numpy as np
import pandas as pd
from keras.models import Model
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from sklearn.metrics import pairwise_distances
from IPython.display import display
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

"""## input image에서 feature extract (with Keras)"""

class RecommendItems:
    def __init__(self):
        base_model = VGG16(weights='imagenet')   # mobileNet 적용 검토
        self.model = Model(inputs = base_model.inputs, outputs = base_model.layers[-2].output)  # mobileNet 적용시 layer input 
            
    def feature_extractor(self, data, batch_size):
        '''
        extract feature from each image in data
        '''
        n_files = len(data)
        
        datagen = image.ImageDataGenerator(rescale=1./255,
                                          rotation_range=90,
                                          zoom_range=[0.3, 0.7],
                                          horizontal_flip=True,
                                          vertical_flip=True)
        generator = datagen.flow_from_dataframe(data,
                                               x_col='Im_file',
                                               target_size=(224, 224),
                                               batch_size=batch_size,
                                               class_mode=None,
                                               shuffle=False)
        
        # output feature 를 각 input 이미지의 장소(label)로 변경하여 진행
        feature = self.model.predict_generator(generator, n_files//batch_size)
        data['feature'] = feature.tolist()
        return data
    

    def dist_calculator(self, data, product_id):
        '''
        calculate the distance between every datapoint and input item
        '''
        # Select random sample having the product_id
        temp = data.copy()
        sample = temp[temp['Id'] == product_id].sample()
        
        # Extract Features
        features = np.vstack(temp['feature'])
        sample_feature = np.asarray(sample.iloc[0, -1])

        # Calculated the cosine distance b/ sample and all of the features
        cos_dist = pairwise_distances(features, sample_feature.reshape(1, -1), metric='cosine')
        temp['cos_dist'] = cos_dist
        temp = temp.sort_values(by=['cos_dist'])
        return temp

re = RecommendItems()

# df = imported data

data = re.feature_extractor(data=df, batch_size=1)

"""## label 예측모델 (with Pytorch)"""

import numpy as np
import pandas as pd
import torch
import torchaudio
from torch.nn import init
from torch import nn, optim
import torch.nn.functional as F
from torch.nn.modules.batchnorm import BatchNorm2d
from torch.utils.data import DataLoader, Dataset, random_split
import os
import librosa
import matplotlib.pyplot as plt
import librosa.display
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict
from itertools import product
from sklearn.preprocessing import LabelEncoder
import random
import time
import warnings
warnings.filterwarnings(action = 'ignore')

# load data

# Convert labels from str to num(int) > temporarily, for training
le = LabelEncoder()
train['id'] = le.fit_transform(train['id'])
train.head()

le.classes_

# 직접 작성한 model이나, pretrained model import 해도 될 듯

class SpeakerIdentifier(nn.Module):
  def __init__(self) :
    super().__init__()
    conv_layers = []

    # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization
    self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
    self.relu1 = nn.ReLU()
    self.bn1 = nn.BatchNorm2d(8)
    init.kaiming_normal_(self.conv1.weight, a=0.1)
    self.conv1.bias.data.zero_()
    conv_layers += [self.conv1, self.relu1, self.bn1]

    # Second Convolution Block
    self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.relu2 = nn.ReLU()
    self.bn2 = nn.BatchNorm2d(16)
    init.kaiming_normal_(self.conv2.weight, a=0.1)
    self.conv2.bias.data.zero_()
    conv_layers += [self.conv2, self.relu2, self.bn2]

    # Third Convolution Block
    self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.relu3 = nn.ReLU()
    self.bn3 = nn.BatchNorm2d(32)
    init.kaiming_normal_(self.conv3.weight, a=0.1)
    self.conv3.bias.data.zero_()
    conv_layers += [self.conv3, self.relu3, self.bn3]

    # Fourth Convolution Block
    self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.relu4 = nn.ReLU()
    self.bn4 = nn.BatchNorm2d(64)
    init.kaiming_normal_(self.conv4.weight, a=0.1)
    self.conv4.bias.data.zero_()
    conv_layers += [self.conv4, self.relu4, self.bn4]
      
    # Fifth Convolution Block
    self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.relu5 = nn.ReLU()
    self.bn5 = nn.BatchNorm2d(128)
    init.kaiming_normal_(self.conv5.weight, a=0.1)
    self.conv5.bias.data.zero_()
    conv_layers += [self.conv5, self.relu5, self.bn5]
        
    # Sixth Convolution Block
    self.conv6 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    self.relu6 = nn.ReLU()
    self.bn6 = nn.BatchNorm2d(256)
    init.kaiming_normal_(self.conv6.weight, a=0.1)
    self.conv6.bias.data.zero_()
    conv_layers += [self.conv6, self.relu6, self.bn6]

    # Linear Classifier
    self.ap = nn.AdaptiveAvgPool2d(output_size=1)
    self.lin = nn.Linear(in_features=256, out_features=3182) # Set output labels

    # Wrap the Convolutional Blocks
    self.model = nn.Sequential(*conv_layers)    
  
  def forward(self, x) :
    x = self.model(x)
    x = self.ap(x)
    x = x.view(x.shape[0], -1)
    x = self.lin(x)

    return x

class SpeakerID_Trainer(nn.Module):
    def __init__(self, model, dataset, opt = "adam", lr = 0.001, n_epochs = 10, has_scheduler = False, device = 'cuda') :
        super().__init__()
        
        self.model = model
        self.n_epochs = n_epochs
        self.criterion = nn.CrossEntropyLoss()
        self._get_optimizer(opt = opt.lower(), lr = lr)
        self.has_scheduler = has_scheduler
        if self.has_scheduler :
            self._get_scheduler()
        
        self.device = device
        
        self.train_data = dataset[0]
        self.valid_data = dataset[1]
        self.test_data = dataset[2]
        
    def _get_optimizer(self, opt, lr = 0.001) :
        if opt == 'sgd' :
            self.optimizer = torch.optim.SGD(params = self.model.parameters(), lr = lr)
        elif opt == 'adam' :
            self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = lr)
        else :
            raise ValueError(f"optimizer {opt} is not supported.")
        
    def _get_scheduler(self):
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer = self.optimizer, max_lr = 0.001,
                                                            steps_per_epoch = int(len(dataset[0])) * self.n_epochs,
                                                            epochs = self.n_epochs, anneal_strategy = 'linear')     
    
    def train(self, train_loader, valid_loader, n_epochs, disp_epoch = 5) :
        print("==============<< Train Start >>==============")
        history = {"train_loss" : [], "train_accuracy" : [], "valid_loss" : [], "valid_accuracy" : []}   
        
        for epoch in range(n_epochs) :
            correct_tr = 0
            total_tr = 0
            loss_tr = 0.0
            start_time = time.time()
            
            # Training
            for i, data in enumerate(train_loader) :
                inputs, labels = data[0].to(device), data[1].to(device)
    
                # Normalize
                inputs_m, inputs_s = inputs.mean(), inputs.std()
                inputs = (inputs - inputs_m) / inputs_s

                # Initiate the parameter gradients
                self.optimizer.zero_grad()

                # forward + backward + optimize
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
                if self.has_scheduler :
                    self.scheduler.step()

                # Stats the loss & accuracy
                loss_tr += loss.item()

                # Get the predicted class with the highest score
                _, prediction = torch.max(outputs, 1)
                correct_tr += (prediction == labels).sum().item()
                total_tr += prediction.shape[0]
                
            # Validation
            correct_val = 0
            total_val = 0
            loss_val = 0.0        
            predictions = []
            
            with torch.no_grad() :
                for data in valid_loader :
                    inputs = data[0].to(self.device)
                    val_labels = data[1].to(self.device)
                
                    inputs_m, inputs_s = inputs.mean(), inputs.std()
                    inputs = (inputs - inputs_m) / inputs_s                    
                    
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, val_labels)
                    loss_val += loss.item()
                    
                    _, prediction = torch.max(outputs, 1)
                    predictions.append(prediction)
                    
                    correct_val += (prediction == val_labels).sum().item()
                    total_val += prediction.shape[0]
            
            accuracy_tr = correct_tr / total_tr
            accuracy_val = correct_val / total_val
            loss_tr = loss_tr / len(train_loader)
            loss_val = loss_val / len(valid_loader)
            
            history['train_loss'].append(loss_tr)
            history['train_accuracy'].append(accuracy_tr)
            history['valid_loss'].append(loss_val)
            history['valid_accuracy'].append(accuracy_val)                 
            
            if epoch % disp_epoch == 0 :
                print(f"Epoch : {epoch} / Tr_loss : {loss_tr:>6f}, Tr_accuracy : {accuracy_tr:>4f}, Val_loss : {loss_val:>6f}, Val_accuracy : {accuracy_val:>4f} / Time : {(time.time() - start_time):>6f}") 

        # Plot the graph of total training
        self.plot_history(history)
        
    def plot_history(self, history) :
        f, ax = plt.subplots(1,2, figsize = (10, 5))
        
        ax[0].plot(history["train_loss"], color='red', label='Train Loss')
        ax[0].plot(history['valid_loss'], color='blue', label='Valid Loss')
        ax[0].legend(loc='upper right')
        ax[0].set_title("LOSS")

        ax[1].plot(history['train_accuracy'], color='red', label='Train Accuracy')
        ax[1].plot(history['valid_accuracy'], color='blue', label='Valid Accuracy')
        ax[1].legend(loc='upper right')
        ax[1].set_title("ACCURACY")
        
        plt.tight_layout()
        plt.show()
    
    def test(self, test_loader) :
        print("==============<< Test Start >>==============")
        test_predictions = []
        results = []
        s_time = time.time()
        with torch.no_grad() :
            for data in tqdm(test_loader) :
                inputs = data[0].to(self.device)
                
                inputs_m, inputs_s = inputs.mean(), inputs.std()
                inputs = (inputs - inputs_m) / inputs_s
                
                outputs = self.model(inputs)
                
                _, prediction = torch.max(outputs, 1)
                test_predictions.append(prediction)
        
        for batch in test_predictions :
            for i in batch :
                results.append(i.item())
        print(f"===<< Test Completed / Time : {(time.time() - s_time):>6f} >>===")
        return results

# loaded data split
train_ds, valid_ds = torch.utils.data.random_split(tr_ds, [n_train, n_valid])
train_dl = torch.utils.data.DataLoader(train_ds, batch_size = 64, shuffle = True)
valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = 64, shuffle = False)
test_dl = torch.utils.data.DataLoader(test_ds, batch_size = 64, shuffle = False)
dataset = [train_dl,valid_dl,test_dl]

si_model = SpeakerIdentifier()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Device: {}".format(device))

si_model.to(device)
next(si_model.parameters()).device

tr = SpeakerID_Trainer(model = si_model, dataset = dataset, opt = 'adam', lr = 0.001, has_scheduler=True, device=device).to(device)

tr.train(train_dl, valid_dl, n_epochs=100, disp_epoch=10)

results = tr.test(test_dl)

test['label'] = results
test['label'] = le.inverse_transform(test['label'])